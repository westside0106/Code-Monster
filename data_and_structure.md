# ğŸ§¬ Data & Structure â€“ data_handling_snippets.py + structure.txt

## ğŸ”¹ Quelle 1: data_handling_snippets.py

```python
import pandas as pd
import json
import csv
from typing import Any, Iterator, Dict, Union

# ğŸ“Š CSV â†’ Pandas DataFrame mit Hygienefunktionen
def load_csv_to_df(
    path: str,
    encoding: str = 'utf-8',
    usecols: list[str] | None = None,
    dtype: dict[str, Any] | None = None,
    parse_dates: list[str] | None = None,
) -> pd.DataFrame:
    df = pd.read_csv(path, encoding=encoding, usecols=usecols, dtype=dtype, parse_dates=parse_dates)
    df.dropna(inplace=True)
    return df

# ğŸ’¾ JSON â†’ Python-Dict
def load_json(path: str, encoding: str = 'utf-8') -> Union[Dict, list]:
    with open(path, 'r', encoding=encoding) as f:
        return json.load(f)

# ğŸ–¨ï¸ JSON-Dump mit Sortierung & Standardformat
def save_json(obj: Any, path: str, encoding: str = 'utf-8', indent: int = 2) -> None:
    with open(path, 'w', encoding=encoding) as f:
        json.dump(obj, f, ensure_ascii=False, indent=indent, sort_keys=True)

# ğŸ” Zeilenweises CSV-Lesen groÃŸer Dateien
def iter_csv(path: str, encoding: str = 'utf-8') -> Iterator[Dict[str, str]]:
    with open(path, 'r', encoding=encoding) as f:
        reader = csv.DictReader(f)
        for row in reader:
            yield row

# ğŸ¯ Flexible Daten-Ouput Funktion
def export_data(
    df: pd.DataFrame,
    target: str,
    format: str = 'csv',
    **kwargs
) -> None:
    match format.lower():
        case 'csv':
            df.to_csv(target, index=False, **kwargs)
        case 'json':
            save_json(df.to_dict(orient='records'), target, **kwargs)
        case 'excel':
            df.to_excel(target, index=False, **kwargs)
        case _:
            raise ValueError(f"Unsupported format: {format}")

# ğŸ§¼ Beispiel fÃ¼r Data-Cleaning Pipeline
def clean_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.drop_duplicates()
    for col in df.select_dtypes(include='object').columns:
        df[col] = df[col].str.strip().str.lower()
    return df

# ğŸ“¦ Testbare Main-Funktion
def main():
    df = load_csv_to_df('data/input.csv', parse_dates=['timestamp'])
    df = clean_df(df)
    export_data(df, 'data/output.json', format='json')
    df_large = pd.DataFrame(iter_csv('data/huge.csv'))
    print("Rows loaded:", len(df_large))

if __name__ == "__main__":
    main()
```

---

## ğŸ”¹ Quelle 2: structure.txt

```
ğŸ“ my_project/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ my_project/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py           # Entry-Point (`python -m my_project`)
â”‚       â”œâ”€â”€ config.py         # Config via `pydantic`/`toml`
â”‚       â”œâ”€â”€ services.py       # Business-Logic
â”‚       â”œâ”€â”€ models.py         # Data-Modelle (z.â€¯B. `@dataclass` oder Pydantic)
â”‚       â”œâ”€â”€ utils.py          # Allgemeine Hilfsfunktionen
â”‚       â””â”€â”€ api/              # (Optional) API-Logik als Submodule
â”‚           â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/                    # Unittests (pytest-konform)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_main.py
â”‚
â”œâ”€â”€ docs/                     # Projektdokumentation (z.B. Sphinx/ReStructuredText)
â”‚
â”œâ”€â”€ scripts/                  # CLI oder Hilfsskripte (z.â€¯B. Data-Migration)
â”‚
â”œâ”€â”€ pyproject.toml            # Tooling-Konfiguration (build, lint, test, format)
â”œâ”€â”€ poetry.lock or requirements.txt  # Lock-Datei / Dependencies
â”œâ”€â”€ tox.ini or .github/workflows/ci.yml  # CI/CD Pipeline-Definition
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```